{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67326,"status":"ok","timestamp":1714456353342,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"JvaEkx8Cyvhg","outputId":"c3d0fc69-8191-4d38-dcef-f8a95eae798b"},"outputs":[],"source":["!pip install open_clip_torch transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["655bb463feda412caeb2d9202e214c2f","ad4d301971fd45afa01430d7b1b6d965","9f4282d1ae91472db9a9d998549fd8ec","677c2488fcb84d3fafda2c5de184da77","66ab67d60b444ee2aa55494243daa549","79def8711ad0429990f2d037dd4d13e1","578245184525463c9ae533056fed10e1","239f27fe078b4a26b0fa8c008c6de841","9fe1e6ca37ba486aa76811868a1395a1","c20bc1fca5284d0faa6a92f3ec59d038","9459b26c753a424c9b5456977ef69d43"]},"executionInfo":{"elapsed":55584,"status":"ok","timestamp":1714456408922,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"vE4lFFkKyotX","outputId":"464ce6aa-b350-4036-ff97-9b76ea062f18"},"outputs":[],"source":["import open_clip\n","import torch\n","\n","model, _, transform = open_clip.create_model_and_transforms(\n","  model_name=\"coca_ViT-L-14\",\n","  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1714456409130,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"oOaE1AmDyth_","outputId":"131a068d-029b-4960-9092-6cccaafbea8c"},"outputs":[],"source":["!wget https://i.imgur.com/8H7XCH0.jpg -O cat.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714456409130,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"Y9Q6bhVA2L01","outputId":"a24c75b2-4f67-49b1-de1f-4e57f95ea274"},"outputs":[],"source":["from IPython.display import Image\n","Image('cat.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"elapsed":88,"status":"ok","timestamp":1714456420840,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"zbZe4uWxa7hf","outputId":"e64f37aa-2435-4b3b-f01a-5684cb3bde63"},"outputs":[],"source":["from IPython.display import Image\n","Image('goggins.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18671,"status":"ok","timestamp":1714456440950,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"byZKXMGzyr5Y","outputId":"39693a64-202a-48c2-a0ad-b7656ea907b1"},"outputs":[],"source":["from PIL import Image\n","im = Image.open(\"goggins.jpg\").convert(\"RGB\")\n","im = transform(im).unsqueeze(0)\n","\n","with torch.no_grad(), torch.cuda.amp.autocast():\n","  generated = model.generate(image=im, generation_type=\"top_p\")\n","\n","print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":96,"status":"ok","timestamp":1714456555035,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"zDuKghhJgU-_"},"outputs":[],"source":["import sys, inspect\n","from open_clip import CoCa"]},{"cell_type":"markdown","metadata":{"id":"gah65i4NkxYe"},"source":["# CoCa source code"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":116,"status":"ok","timestamp":1714456742001,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"p2Ic9cqElafr"},"outputs":[],"source":["# imports\n","from typing import Optional\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","import numpy as np\n","from dataclasses import dataclass\n","\n","from open_clip.transformer import (\n","    LayerNormFp32,\n","    LayerNorm,\n","    QuickGELU,\n","    MultimodalTransformer,\n",")\n","from open_clip.model import CLIPTextCfg, CLIPVisionCfg, _build_vision_tower, _build_text_tower\n","\n","try:\n","    from transformers import (\n","        BeamSearchScorer,\n","        LogitsProcessorList,\n","        TopPLogitsWarper,\n","        TopKLogitsWarper,\n","        RepetitionPenaltyLogitsProcessor,\n","        MinLengthLogitsProcessor,\n","        MaxLengthCriteria,\n","        StoppingCriteriaList\n","    )\n","\n","    GENERATION_TYPES = {\n","        \"top_k\": TopKLogitsWarper,\n","        \"top_p\": TopPLogitsWarper,\n","        \"beam_search\": \"beam_search\"\n","    }\n","    _has_transformers = True\n","except ImportError as e:\n","    GENERATION_TYPES = {\n","        \"top_k\": None,\n","        \"top_p\": None,\n","        \"beam_search\": \"beam_search\"\n","    }\n","    _has_transformers = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1714456409376,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"c3COwyfElPkQ"},"outputs":[],"source":["\n","\n","\n","@dataclass\n","class MultimodalCfg(CLIPTextCfg):\n","    mlp_ratio: int = 4\n","    dim_head: int = 64\n","    heads: int = 8\n","    n_queries: int = 256\n","    attn_pooler_heads: int = 8\n","\n","\n","def _build_text_decoder_tower(\n","        embed_dim,\n","        multimodal_cfg,\n","        quick_gelu: bool = False,\n","        cast_dtype: Optional[torch.dtype] = None,\n","):\n","    multimodal_cfg = MultimodalCfg(**multimodal_cfg) if isinstance(multimodal_cfg, dict) else multimodal_cfg\n","    act_layer = QuickGELU if quick_gelu else nn.GELU\n","    norm_layer = (\n","        LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n","    )\n","\n","    decoder = MultimodalTransformer(\n","        context_length=multimodal_cfg.context_length,\n","        width=multimodal_cfg.width,\n","        heads=multimodal_cfg.heads,\n","        layers=multimodal_cfg.layers,\n","        ls_init_value=multimodal_cfg.ls_init_value,\n","        output_dim=embed_dim,\n","        act_layer=act_layer,\n","        norm_layer=norm_layer,\n","    )\n","\n","    return decoder\n","\n","\n","class CoCa(nn.Module):\n","    def __init__(\n","            self,\n","            embed_dim,\n","            multimodal_cfg: MultimodalCfg,\n","            text_cfg: CLIPTextCfg,\n","            vision_cfg: CLIPVisionCfg,\n","            quick_gelu: bool = False,\n","            init_logit_scale: float = np.log(1 / 0.07),\n","            init_logit_bias: Optional[float] = None,\n","            cast_dtype: Optional[torch.dtype] = None,\n","            pad_id: int = 0,\n","    ):\n","        super().__init__()\n","        multimodal_cfg = MultimodalCfg(**multimodal_cfg) if isinstance(multimodal_cfg, dict) else multimodal_cfg\n","        text_cfg = CLIPTextCfg(**text_cfg) if isinstance(text_cfg, dict) else text_cfg\n","        vision_cfg = CLIPVisionCfg(**vision_cfg) if isinstance(vision_cfg, dict) else vision_cfg\n","\n","        self.text = _build_text_tower(\n","            embed_dim=embed_dim,\n","            text_cfg=text_cfg,\n","            quick_gelu=quick_gelu,\n","            cast_dtype=cast_dtype,\n","        )\n","\n","        vocab_size = (\n","            text_cfg.vocab_size  # for hf models\n","            if hasattr(text_cfg, \"hf_model_name\") and text_cfg.hf_model_name is not None\n","            else text_cfg.vocab_size\n","        )\n","\n","        self.visual = _build_vision_tower(\n","            embed_dim=embed_dim,\n","            vision_cfg=vision_cfg,\n","            quick_gelu=quick_gelu,\n","            cast_dtype=cast_dtype,\n","        )\n","\n","        self.text_decoder = _build_text_decoder_tower(\n","            vocab_size,\n","            multimodal_cfg=multimodal_cfg,\n","            quick_gelu=quick_gelu,\n","            cast_dtype=cast_dtype,\n","        )\n","\n","        self.logit_scale = nn.Parameter(torch.ones([]) * init_logit_scale)\n","        if init_logit_bias is not None:\n","            self.logit_bias = nn.Parameter(torch.ones([]) * init_logit_bias)\n","        else:\n","            self.logit_bias = None\n","        self.pad_id = pad_id\n","\n","        self.context_length = multimodal_cfg.context_length\n","\n","    @torch.jit.ignore\n","    def set_grad_checkpointing(self, enable: bool = True):\n","        self.visual.set_grad_checkpointing(enable)\n","        self.text.set_grad_checkpointing(enable)\n","        self.text_decoder.set_grad_checkpointing(enable)\n","\n","    def _encode_image(self, images, normalize: bool = True):\n","        image_latent, tokens_embs = self.visual(images)\n","        image_latent = F.normalize(image_latent, dim=-1) if normalize else image_latent\n","        return image_latent, tokens_embs\n","\n","    def _encode_text(self, text, normalize: bool = True):\n","        text_latent, token_emb = self.text(text)\n","        text_latent = F.normalize(text_latent, dim=-1) if normalize else text_latent\n","        return text_latent, token_emb\n","\n","    def encode_image(self, images, normalize: bool = True):\n","        image_latent, _ = self._encode_image(images, normalize=normalize)\n","        return image_latent\n","\n","    def encode_text(self, text, normalize: bool = True):\n","        text_latent, _ = self._encode_text(text, normalize=normalize)\n","        return text_latent\n","\n","    def forward(\n","            self,\n","            image,\n","            text: Optional[torch.Tensor] = None,\n","            image_latent: Optional[torch.Tensor] = None,\n","            image_embs: Optional[torch.Tensor] = None,\n","    ):\n","        if image_latent is None or image_embs is None:\n","            image_latent, image_embs = self._encode_image(image)\n","\n","        if text is None:\n","            return {\"image_features\": image_latent, \"image_embs\": image_embs}\n","\n","        text_latent, token_embs = self._encode_text(text)\n","\n","        # TODO: add assertion to avoid bugs?\n","        labels = text[:, -token_embs.shape[1]:]\n","\n","        logits = self.text_decoder(image_embs, token_embs)\n","        out_dict = {\n","            \"image_features\": image_latent,\n","            \"text_features\": text_latent,\n","            \"logits\": logits,\n","            \"labels\": labels,\n","            \"logit_scale\": self.logit_scale.exp()\n","        }\n","        if self.logit_bias is not None:\n","            out_dict[\"logit_bias\"] = self.logit_bias\n","        return out_dict\n","\n","    def generate(\n","        self,\n","        image,\n","        text=None,\n","        seq_len=30,\n","        max_seq_len=77,\n","        temperature=1.,\n","        generation_type=\"beam_search\",\n","        top_p=0.1,  # keep tokens in the 1 - top_p quantile\n","        top_k=1,  # keeps the top_k most probable tokens\n","        pad_token_id=None,\n","        eos_token_id=None,\n","        sot_token_id=None,\n","        num_beams=6,\n","        num_beam_groups=3,\n","        min_seq_len=5,\n","        stopping_criteria=None,\n","        repetition_penalty=1.0,\n","        fixed_output_length=False # if True output.shape == (batch_size, seq_len)\n","    ):\n","        # taking many ideas and components from HuggingFace GenerationMixin\n","        # https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n","        assert _has_transformers, \"Please install transformers for generate functionality. `pip install transformers`.\"\n","        assert seq_len > min_seq_len, \"seq_len must be larger than min_seq_len\"\n","\n","        with torch.no_grad():\n","            sot_token_id = 49406 if sot_token_id is None else sot_token_id\n","            eos_token_id = 49407 if eos_token_id is None else eos_token_id\n","            pad_token_id = self.pad_id if pad_token_id is None else pad_token_id\n","            logit_processor = LogitsProcessorList(\n","                [\n","                    MinLengthLogitsProcessor(min_seq_len, eos_token_id),\n","                    RepetitionPenaltyLogitsProcessor(repetition_penalty),\n","                ]\n","            )\n","\n","            if stopping_criteria is None:\n","                stopping_criteria = [MaxLengthCriteria(max_length=seq_len)]\n","\n","            stopping_criteria = StoppingCriteriaList(\n","                stopping_criteria\n","            )\n","\n","            device = image.device\n","\n","            if generation_type == \"beam_search\":\n","                output = self._generate_beamsearch(\n","                    image_inputs=image,\n","                    pad_token_id=pad_token_id,\n","                    eos_token_id=eos_token_id,\n","                    sot_token_id=sot_token_id,\n","                    num_beams=num_beams,\n","                    num_beam_groups=num_beam_groups,\n","                    min_seq_len=min_seq_len,\n","                    stopping_criteria=stopping_criteria,\n","                    logit_processor=logit_processor,\n","                )\n","                if fixed_output_length and output.shape[1] < seq_len:\n","                    return torch.cat(\n","                        (output, torch.ones(output.shape[0], seq_len-output.shape[1], device=device, dtype=output.dtype) * self.pad_id),\n","                        dim=1\n","                    )\n","                return output\n","\n","            elif generation_type == \"top_p\":\n","                logit_warper = GENERATION_TYPES[generation_type](top_p)\n","            elif generation_type == \"top_k\":\n","                logit_warper = GENERATION_TYPES[generation_type](top_k)\n","            else:\n","                raise ValueError(\n","                    f\"generation_type has to be one of \"\n","                    f\"{'| ' + ' | '.join(list(GENERATION_TYPES.keys())) + ' |'}.\"\n","                )\n","\n","            image_latent, image_embs = self._encode_image(image)\n","\n","            if text is None:\n","                text = torch.ones((image.shape[0], 1), device=device, dtype=torch.long) * sot_token_id\n","\n","            was_training = self.training\n","            num_dims = len(text.shape)\n","\n","            if num_dims == 1:\n","                text = text[None, :]\n","\n","            cur_len = text.shape[1]\n","            self.eval()\n","            out = text\n","\n","            while True:\n","                x = out[:, -max_seq_len:]\n","                cur_len = x.shape[1]\n","                logits = self(image, x, image_latent=image_latent, image_embs=image_embs)[\"logits\"][:, -1]\n","                mask = (out[:, -1] == eos_token_id) | (out[:, -1] == pad_token_id)\n","                sample = torch.ones((out.shape[0], 1), device=device, dtype=torch.long) * pad_token_id\n","\n","                if mask.all():\n","                    if not fixed_output_length:\n","                        break\n","                else:\n","                    logits = logits[~mask, :]\n","                    filtered_logits = logit_processor(x[~mask, :], logits)\n","                    filtered_logits = logit_warper(x[~mask, :], filtered_logits)\n","                    probs = F.softmax(filtered_logits / temperature, dim=-1)\n","\n","                    if (cur_len + 1 == seq_len):\n","                        sample[~mask, :] = torch.ones((sum(~mask), 1), device=device, dtype=torch.long) * eos_token_id\n","                    else:\n","                        sample[~mask, :] = torch.multinomial(probs, 1)\n","\n","                out = torch.cat((out, sample), dim=-1)\n","\n","                cur_len += 1\n","\n","                if stopping_criteria(out, None):\n","                    break\n","\n","            if num_dims == 1:\n","                out = out.squeeze(0)\n","\n","            self.train(was_training)\n","            return out\n","\n","    def _generate_beamsearch(\n","            self,\n","            image_inputs,\n","            pad_token_id=None,\n","            eos_token_id=None,\n","            sot_token_id=None,\n","            num_beams=6,\n","            num_beam_groups=3,\n","            min_seq_len=5,\n","            stopping_criteria=None,\n","            logit_processor=None,\n","            logit_warper=None,\n","    ):\n","        device = image_inputs.device\n","        batch_size = image_inputs.shape[0]\n","        image_inputs = torch.repeat_interleave(image_inputs, num_beams, dim=0)\n","        image_latent, image_embs = self._encode_image(image_inputs)\n","\n","        input_ids = torch.ones((batch_size * num_beams, 1), device=device, dtype=torch.long)\n","        input_ids = input_ids * sot_token_id\n","        beam_scorer = BeamSearchScorer(\n","            batch_size=batch_size,\n","            num_beams=num_beams,\n","            device=device,\n","            num_beam_groups=num_beam_groups,\n","        )\n","        # instantiate logits processors\n","        logits_processor = (\n","            LogitsProcessorList([MinLengthLogitsProcessor(min_seq_len, eos_token_id=eos_token_id)])\n","            if logit_processor is None\n","            else logit_processor\n","        )\n","\n","        num_beams = beam_scorer.num_beams\n","        num_beam_groups = beam_scorer.num_beam_groups\n","        num_sub_beams = num_beams // num_beam_groups\n","        batch_size = len(beam_scorer._beam_hyps) // num_beam_groups\n","        batch_beam_size, cur_len = input_ids.shape\n","        beam_indices = None\n","\n","        if num_beams * batch_size != batch_beam_size:\n","            raise ValueError(\n","                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n","            )\n","\n","        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n","        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in\n","        # the same group don't produce same tokens everytime.\n","        beam_scores[:, ::num_sub_beams] = 0\n","        beam_scores = beam_scores.view((batch_size * num_beams,))\n","\n","        while True:\n","\n","            # predicted tokens in cur_len step\n","            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n","\n","            # indices which will form the beams in the next time step\n","            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n","\n","            # do one decoder step on all beams of all sentences in batch\n","            model_inputs = prepare_inputs_for_generation(input_ids=input_ids, image_inputs=image_inputs)\n","            outputs = self(\n","                model_inputs['images'],\n","                model_inputs['text'],\n","                image_latent=image_latent,\n","                image_embs=image_embs\n","            )\n","\n","            for beam_group_idx in range(num_beam_groups):\n","                group_start_idx = beam_group_idx * num_sub_beams\n","                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n","                group_size = group_end_idx - group_start_idx\n","\n","                # indices of beams of current group among all sentences in batch\n","                batch_group_indices = []\n","\n","                for batch_idx in range(batch_size):\n","                    batch_group_indices.extend(\n","                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n","                    )\n","                group_input_ids = input_ids[batch_group_indices]\n","\n","                # select outputs of beams of currentg group only\n","                next_token_logits = outputs['logits'][batch_group_indices, -1, :]\n","                vocab_size = next_token_logits.shape[-1]\n","\n","                next_token_scores_processed = logits_processor(\n","                    group_input_ids, next_token_logits, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n","                )\n","                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n","                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n","\n","                # reshape for beam search\n","                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n","\n","                next_token_scores, next_tokens = torch.topk(\n","                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True\n","                )\n","\n","                next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n","                next_tokens = next_tokens % vocab_size\n","\n","                # stateless\n","                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n","                beam_outputs = beam_scorer.process(\n","                    group_input_ids,\n","                    next_token_scores,\n","                    next_tokens,\n","                    next_indices,\n","                    pad_token_id=pad_token_id,\n","                    eos_token_id=eos_token_id,\n","                    beam_indices=process_beam_indices,\n","                    group_index=beam_group_idx,\n","                )\n","                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n","                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n","                beam_idx = beam_outputs[\"next_beam_indices\"]\n","\n","                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n","                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n","                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n","\n","                # (beam_idx // group_size) -> batch_idx\n","                # (beam_idx % group_size) -> offset of idx inside the group\n","                reordering_indices[batch_group_indices] = (\n","                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\") + group_start_idx + (beam_idx % group_size)\n","                )\n","\n","            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n","\n","            # increase cur_len\n","            cur_len = cur_len + 1\n","            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n","                break\n","\n","        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n","        sequence_outputs = beam_scorer.finalize(\n","            input_ids,\n","            beam_scores,\n","            next_tokens,\n","            next_indices,\n","            pad_token_id=pad_token_id,\n","            eos_token_id=eos_token_id,\n","            max_length=stopping_criteria.max_length,\n","            beam_indices=final_beam_indices,\n","        )\n","        return sequence_outputs['sequences']\n","\n","\n","def prepare_inputs_for_generation(input_ids, image_inputs, past=None, **kwargs):\n","    if past:\n","        input_ids = input_ids[:, -1].unsqueeze(-1)\n","\n","    attention_mask = kwargs.get(\"attention_mask\", None)\n","    position_ids = kwargs.get(\"position_ids\", None)\n","\n","    if attention_mask is not None and position_ids is None:\n","        # create position_ids on the fly for batch generation\n","        position_ids = attention_mask.long().cumsum(-1) - 1\n","        position_ids.masked_fill_(attention_mask == 0, 1)\n","    else:\n","        position_ids = None\n","    return {\n","        \"text\": input_ids,\n","        \"images\": image_inputs,\n","        \"past_key_values\": past,\n","        \"position_ids\": position_ids,\n","        \"attention_mask\": attention_mask,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1714456409377,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"3FFb_3lsT8U1"},"outputs":[],"source":["from clip_benchmark.datasets.builder import build_dataset\n","import pandas as pd\n","import os\n","\n","root_path = \"path/to/data/dir\" # set this to smth meaningful\n","ds = build_dataset(\"mscoco_captions\", root=root_path, split=\"train\") # this downloads the dataset if it is not there already\n","coco = ds.coco\n","imgs = coco.loadImgs(coco.getImgIds())\n","future_df = {\"filepath\":[], \"title\":[]}\n","for img in imgs:\n","    caps = coco.imgToAnns[img[\"id\"]]\n","    for cap in caps:\n","        future_df[\"filepath\"].append(img[\"file_name\"])\n","        future_df[\"title\"].append(cap[\"caption\"])\n","pd.DataFrame.from_dict(future_df).to_csv(\n","  os.path.join(root_path, \"train2014.csv\"), index=False, sep=\"\\t\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"GtJqqdgEk3Gy"},"source":["# VideoCoCaBaseline"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":124,"status":"ok","timestamp":1714456745470,"user":{"displayName":"Sarthak Mohanty","userId":"17264248886235296473"},"user_tz":240},"id":"IpQIKoQzjsC5"},"outputs":[],"source":["class VideoCoCaBaseline(nn.Module):\n","    def __init__(self, coca_model):\n","        super(VideoCoCaBaseline, self).__init__()\n","        self.coca = coca_model  # Assume this is a trained instance of CoCa\n","\n","    def forward(self, video_frames):\n","        \"\"\"\n","        Forward pass to encode a video by averaging the embeddings of its frames.\n","\n","        :param video_frames: Tensor of shape (batch_size, num_frames, C, H, W)\n","        :return: Averaged video embeddings of shape (batch_size, embed_dim)\n","        \"\"\"\n","        batch_size, num_frames, C, H, W = video_frames.shape\n","        video_frames = video_frames.view(-1, C, H, W)  # Flatten to (batch_size * num_frames, C, H, W)\n","        embeddings, _ = self.coca._encode_image(video_frames)  # Get embeddings from CoCa model\n","        embeddings = embeddings.view(batch_size, num_frames, -1).mean(dim=1)  # Average embeddings per video\n","        return embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iTTxW3CkdVl"},"outputs":[],"source":["baseline_video_model = VideoCoCaBaseline(model)\n","\n","from PIL import Image\n","im = Image.open(\"goggins.jpg\").convert(\"RGB\")\n","im = transform(im).unsqueeze(0)\n","\n","#\n","\n","with torch.no_grad(), torch.cuda.amp.autocast():\n","  generated = baseline_video_model.generate(image=im, generation_type=\"top_p\")\n","\n","print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb","timestamp":1714436805481}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"239f27fe078b4a26b0fa8c008c6de841":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"578245184525463c9ae533056fed10e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"655bb463feda412caeb2d9202e214c2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad4d301971fd45afa01430d7b1b6d965","IPY_MODEL_9f4282d1ae91472db9a9d998549fd8ec","IPY_MODEL_677c2488fcb84d3fafda2c5de184da77"],"layout":"IPY_MODEL_66ab67d60b444ee2aa55494243daa549"}},"66ab67d60b444ee2aa55494243daa549":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677c2488fcb84d3fafda2c5de184da77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c20bc1fca5284d0faa6a92f3ec59d038","placeholder":"​","style":"IPY_MODEL_9459b26c753a424c9b5456977ef69d43","value":" 2.55G/2.55G [00:28&lt;00:00, 87.2MB/s]"}},"79def8711ad0429990f2d037dd4d13e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9459b26c753a424c9b5456977ef69d43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f4282d1ae91472db9a9d998549fd8ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_239f27fe078b4a26b0fa8c008c6de841","max":2554107511,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9fe1e6ca37ba486aa76811868a1395a1","value":2554107511}},"9fe1e6ca37ba486aa76811868a1395a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad4d301971fd45afa01430d7b1b6d965":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79def8711ad0429990f2d037dd4d13e1","placeholder":"​","style":"IPY_MODEL_578245184525463c9ae533056fed10e1","value":"open_clip_pytorch_model.bin: 100%"}},"c20bc1fca5284d0faa6a92f3ec59d038":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
